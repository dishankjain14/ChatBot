{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT TO SPEECH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyttsx3\n",
    "import pyttsx3\n",
    "text_speech= pyttsx3.init()\n",
    "\n",
    "ans=\"AI Chat is an AI chatbot that writes text. You can use it to write stories, messages, or programming code. You can use the AI chatbot as a virtual tutor in almost any subject.\"\n",
    "ans = 'HELLLO '\n",
    "text_speech.say(ans)\n",
    "text_speech.runAndWait()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPEECH RECOGNATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyaudio\n",
    "# !pip install SpeechRecognition\n",
    "import speech_recognition as sr\n",
    "r=sr.Recognizer()\n",
    "def speak():\n",
    "    r=sr.Recognizer()\n",
    "\n",
    "    with sr.Microphone() as source:\n",
    "        r.adjust_for_ambient_noise(source)\n",
    "        print(\"Say\")\n",
    "        audio=r.listen(source)\n",
    "        try:\n",
    "            print(r.recognize_google(audio,language='hi-In'))\n",
    "            return r.recognize_google(audio)\n",
    "        except Exception as e:\n",
    "            print(\"error\"+str(e))\n",
    "            return \"error\"+str(e)\n",
    "\n",
    "#speak()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        कार्ल हेनरिक उलरिच ने इस वर्गीकरण योजना को कब ...\n",
       "1        एक जीव जो सर्केडियन लय का उपयोग करता है वह अपन...\n",
       "2        किस टेनेसी शहर में अफ्रीकी-अमेरिकी आबादी सबसे ...\n",
       "3                गौतम बुद्ध के प्रवचनों में क्या शामिल है?\n",
       "4                        किस पार्टी का नानजिंग पर शासन है?\n",
       "                               ...                        \n",
       "69974                    शिखर सम्मेलनों की जलवायु कैसी है?\n",
       "69975                             मैकडरमॉट को कब चुना गया?\n",
       "69976    नया लिवरपूल एफसी स्टेडियम 1879 में कहाँ स्थित था?\n",
       "69977    नमक के अलावा माली में और कौन-से बड़े प्राकृतिक...\n",
       "69978    वानूआतू में किस देश से मिशनरियों ने प्रेसबिटेर...\n",
       "Name: question, Length: 69979, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"hindiQuestion.csv\")\n",
    "df=df.drop([\"squad_id\"],axis=\"columns\")\n",
    "\n",
    "# pattern = r'[^\\d\\s\\u0900-\\u097F]+'\n",
    "df['question']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "कार्ल हेनरिक उलरिच ने इस वर्गीकरण योजना को कब विकसित किया?! ? ; ::: + कार्ल ; हेनरिक उलरिच ने इस वर्गीकरण योजना को कब;; विकसित किया   \n",
      "कार्ल हेनरिक उलरिच ने इस वर्गीकरण योजना को कब विकसित किया     कार्ल  हेनरिक उलरिच ने इस वर्गीकरण योजना को कब विकसित किया   \n",
      "['कार्ल', 'हेनरिक', 'उलरिच', 'ने', 'इस', 'वर्गीकरण', 'योजना', 'को', 'कब', 'विकसित', 'किया', 'कार्ल', 'हेनरिक', 'उलरिच', 'ने', 'इस', 'वर्गीकरण', 'योजना', 'को', 'कब', 'विकसित', 'किया']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ने'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'[^\\d\\s\\u0900-\\u097F]+'\n",
    "#trial\n",
    "test = df[\"question\"][0]+\"! ? ; ::: + कार्ल ; हेनरिक उलरिच ने इस वर्गीकरण योजना को कब;; विकसित किया   \"\n",
    "print(test)\n",
    "test = re.sub(pattern, '', test )\n",
    "print(test)\n",
    "\n",
    "\n",
    "\n",
    "##tokenize\n",
    "from nltk.tokenize import word_tokenize,RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "print(word_tokenize(test))\n",
    "wnl.lemmatize(word_tokenize(test)[3])\n",
    "\n",
    "\n",
    "#trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPERATION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>Answers_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>१८६० के दशक में</td>\n",
       "      <td>1860 के दशक में कार्ल हेनरिक उलरिच द्वारा निजी...</td>\n",
       "      <td>कार्ल हेनरिक उलरिच वर्गीकरण योजना विकसित</td>\n",
       "      <td>50801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>संसाधन</td>\n",
       "      <td>इस प्रकार वे जीवों को उन लोगों की तुलना में पर...</td>\n",
       "      <td>जीव सर्केडियन लय उपयोग लाभ के उपयोग नहीं</td>\n",
       "      <td>45498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>मेम्फिस</td>\n",
       "      <td>लोकतांत्रिक ताकत ज्यादातर चार प्रमुख शहरों के ...</td>\n",
       "      <td>टेनेसी शहर अफ्रीकी अमेरिकी आबादी सबसे अधिक</td>\n",
       "      <td>37361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>सुता पिटक</td>\n",
       "      <td>\"\" \"प्यारेली त्रिपिटक\" \", जिसका अर्थ\" \"तीन टोक...</td>\n",
       "      <td>गौतम बुद्ध के प्रवचनों शामिल</td>\n",
       "      <td>47799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>सीपीसी</td>\n",
       "      <td>\"वर्तमान में, नानजिंग की सरकार का पूरा नाम\" \"न...</td>\n",
       "      <td>पार्टी नानजिंग शासन</td>\n",
       "      <td>47693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69974</th>\n",
       "      <td>69975</td>\n",
       "      <td>कोई भी शिखर स्थायी बर्फ के क्षेत्र में नहीं पह...</td>\n",
       "      <td>कोई भी शिखर स्थायी बर्फ के क्षेत्र में नहीं पह...</td>\n",
       "      <td>शिखर सम्मेलनों जलवायु कैसी</td>\n",
       "      <td>17812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69975</th>\n",
       "      <td>69976</td>\n",
       "      <td>1988</td>\n",
       "      <td>संघीय रूप से, सिएटल वाशिंगटन के 7 वें कांग्रेस...</td>\n",
       "      <td>मैकडरमॉट चुना</td>\n",
       "      <td>3088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69976</th>\n",
       "      <td>69977</td>\n",
       "      <td>स्टेनली पार्क</td>\n",
       "      <td>एवर्टन मूल रूप से स्टेनली पार्क के दक्षिण-पूर्...</td>\n",
       "      <td>नया लिवरपूल एफसी स्टेडियम 1879 स्थित</td>\n",
       "      <td>48917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69977</th>\n",
       "      <td>69978</td>\n",
       "      <td>प्रमुख प्राकृतिक संसाधनों में सोना शामिल है</td>\n",
       "      <td>माली के प्रमुख प्राकृतिक संसाधनों में से कुछ म...</td>\n",
       "      <td>नमक के अलावा माली प्राकृतिक संसाधन</td>\n",
       "      <td>31226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69978</th>\n",
       "      <td>69979</td>\n",
       "      <td>स्कॉटलैंड</td>\n",
       "      <td>पीसीवी को स्कॉटलैंड से मिशनरियों द्वारा वानूआत...</td>\n",
       "      <td>वानूआतू देश मिशनरियों प्रेसबिटेरियन चर्च शुरूआत</td>\n",
       "      <td>48767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69979 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                             answer  \\\n",
       "0          1                                    १८६० के दशक में   \n",
       "1          2                                             संसाधन   \n",
       "2          3                                            मेम्फिस   \n",
       "3          4                                          सुता पिटक   \n",
       "4          5                                             सीपीसी   \n",
       "...      ...                                                ...   \n",
       "69974  69975  कोई भी शिखर स्थायी बर्फ के क्षेत्र में नहीं पह...   \n",
       "69975  69976                                               1988   \n",
       "69976  69977                                      स्टेनली पार्क   \n",
       "69977  69978        प्रमुख प्राकृतिक संसाधनों में सोना शामिल है   \n",
       "69978  69979                                          स्कॉटलैंड   \n",
       "\n",
       "                                                 context  \\\n",
       "0      1860 के दशक में कार्ल हेनरिक उलरिच द्वारा निजी...   \n",
       "1      इस प्रकार वे जीवों को उन लोगों की तुलना में पर...   \n",
       "2      लोकतांत्रिक ताकत ज्यादातर चार प्रमुख शहरों के ...   \n",
       "3      \"\" \"प्यारेली त्रिपिटक\" \", जिसका अर्थ\" \"तीन टोक...   \n",
       "4      \"वर्तमान में, नानजिंग की सरकार का पूरा नाम\" \"न...   \n",
       "...                                                  ...   \n",
       "69974  कोई भी शिखर स्थायी बर्फ के क्षेत्र में नहीं पह...   \n",
       "69975  संघीय रूप से, सिएटल वाशिंगटन के 7 वें कांग्रेस...   \n",
       "69976  एवर्टन मूल रूप से स्टेनली पार्क के दक्षिण-पूर्...   \n",
       "69977  माली के प्रमुख प्राकृतिक संसाधनों में से कुछ म...   \n",
       "69978  पीसीवी को स्कॉटलैंड से मिशनरियों द्वारा वानूआत...   \n",
       "\n",
       "                                              question  Answers_Code  \n",
       "0             कार्ल हेनरिक उलरिच वर्गीकरण योजना विकसित         50801  \n",
       "1             जीव सर्केडियन लय उपयोग लाभ के उपयोग नहीं         45498  \n",
       "2           टेनेसी शहर अफ्रीकी अमेरिकी आबादी सबसे अधिक         37361  \n",
       "3                         गौतम बुद्ध के प्रवचनों शामिल         47799  \n",
       "4                                  पार्टी नानजिंग शासन         47693  \n",
       "...                                                ...           ...  \n",
       "69974                       शिखर सम्मेलनों जलवायु कैसी         17812  \n",
       "69975                                    मैकडरमॉट चुना          3088  \n",
       "69976             नया लिवरपूल एफसी स्टेडियम 1879 स्थित         48917  \n",
       "69977               नमक के अलावा माली प्राकृतिक संसाधन         31226  \n",
       "69978  वानूआतू देश मिशनरियों प्रेसबिटेरियन चर्च शुरूआत         48767  \n",
       "\n",
       "[69979 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def data_prep(text):\n",
    "    \n",
    "    text = re.sub(r'[^\\d\\s\\u0900-\\u097F]+', ' ', text )\n",
    "    text=\" \".join(t for t in text.split(\" \") if t not in hindi_stopwords)\n",
    "    return text\n",
    "hindi_stopwords = open('final_stopwords.txt','r', encoding=\"utf8\")\n",
    "hindi_stopwords=(hindi_stopwords.read()).split('\\n')\n",
    "df[\"question\"] = df[\"question\"].apply(data_prep)\n",
    "\n",
    "\n",
    "tf=TfidfVectorizer()\n",
    "tf_train=tf.fit_transform(df[\"question\"])\n",
    "df_check=pd.DataFrame(tf_train.toarray(),columns=tf.get_feature_names_out())\n",
    "\n",
    "le= LabelEncoder()\n",
    "df[\"Answers_Code\"]=le.fit_transform(df[\"answer\"])\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 98.9 MiB for an array with shape (6998, 1853) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m tf \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m      6\u001b[0m tf_train \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfit_transform(df_sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# Transform the sampled questions\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df_check \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mtf_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mget_feature_names_out())  \u001b[38;5;66;03m# Create a DataFrame from the sparse matrix\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Encode the answers\u001b[39;00m\n\u001b[0;32m     10\u001b[0m le \u001b[38;5;241m=\u001b[39m LabelEncoder()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1101\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1101\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\sparse\\_base.py:1327\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 98.9 MiB for an array with shape (6998, 1853) and data type float64"
     ]
    }
   ],
   "source": [
    "# Sample 40% of the dataset\n",
    "df_sample = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Vectorize the sampled questions\n",
    "tf = TfidfVectorizer()\n",
    "tf_train = tf.fit_transform(df_sample[\"question\"])  # Transform the sampled questions\n",
    "df_check = pd.DataFrame(tf_train.toarray(), columns=tf.get_feature_names_out())  # Create a DataFrame from the sparse matrix\n",
    "\n",
    "# Encode the answers\n",
    "le = LabelEncoder()\n",
    "df_sample[\"Answers_Code\"] = le.fit_transform(df_sample[\"answer\"])  # Encode the answers in the sampled DataFrame\n",
    "\n",
    "# Create and fit the Multinomial Naive Bayes model\n",
    "mn = MultinomialNB()\n",
    "mn.fit(df_check, df_sample[\"Answers_Code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create your model (as an example, you may already have your model trained)\n",
    "mn = MultinomialNB()\n",
    "\n",
    "# Assume df_check and df[\"Answers_Code\"] are defined and populated\n",
    "# Uncomment the following line if you need to fit your model\n",
    "# mn.fit(df_check, df[\"Answers_Code\"])\n",
    "\n",
    "# Define the directory and file path\n",
    "directory = \"Dishank_Hindi\"\n",
    "filepath = os.path.join(directory, \"trainedModel\")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "with open(filepath, 'wb') as file:\n",
    "    pickle.dump(mn, file)\n",
    "\n",
    "# Fetching the trained model later\n",
    "with open(filepath, 'rb') as file:\n",
    "    mn_loaded = pickle.load(file)\n",
    "\n",
    "# Now, mn_loaded can be used for predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the trained model\n",
    "model_filepath = r\"C:\\Users\\disha\\Desktop\\hindi chatbot\\ChatbotHindi\\Dishank_Hindi\\trainedModel\"\n",
    "with open(model_filepath, 'rb') as file:\n",
    "    mn_loaded = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample user input\n",
    "userInput = ['टेनेसी शहर']\n",
    "\n",
    "# Preprocess the user input\n",
    "def data_prep(text):\n",
    "    text = re.sub(r'[^\\d\\s\\u0900-\\u097F]+', ' ', text)\n",
    "    text = \" \".join(t for t in text.split(\" \") if t not in hindi_stopwords)\n",
    "    return text\n",
    "\n",
    "# Transform the input text\n",
    "processed_input = data_prep(userInput[0])\n",
    "\n",
    "# Transform using TF-IDF vectorizer\n",
    "testing = tf.transform([processed_input])  # Transforming a list for the vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model_filepath = r\"C:\\Users\\disha\\Desktop\\hindi chatbot\\ChatbotHindi\\Dishank_Hindi\\trainedModel\"\n",
    "with open(model_filepath, 'rb') as file:\n",
    "    mn_loaded = pickle.load(file)\n",
    "\n",
    "# Example user input\n",
    "userInput = ['टेनेसी शहर']\n",
    "\n",
    "# Preprocess the input using the same method used for the training data\n",
    "testing = tf.transform(userInput)  # Ensure 'tf' is defined as TfidfVectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This MultinomialNB instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make predictions using the loaded model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mmn_loaded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtesting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\naive_bayes.py:100\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m    Perform classification on an array of test vectors X.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m        Predicted target values for X.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X(X)\n\u001b[0;32m    102\u001b[0m     jll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_joint_log_likelihood(X)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1622\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1619\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[0;32m   1621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This MultinomialNB instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# Make predictions using the loaded model\n",
    "res = mn_loaded.predict(testing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This MultinomialNB instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m testing \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtransform(userInput)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Make predictions using the trained model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mmn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtesting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Retrieve unique contexts and answer codes from the DataFrame\u001b[39;00m\n\u001b[0;32m     13\u001b[0m Ans \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\naive_bayes.py:100\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m    Perform classification on an array of test vectors X.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m        Predicted target values for X.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X(X)\n\u001b[0;32m    102\u001b[0m     jll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_joint_log_likelihood(X)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1622\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1619\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[0;32m   1621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This MultinomialNB instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# Assuming necessary libraries and previous code (model training, etc.) have been defined\n",
    "\n",
    "# User input\n",
    "userInput = ['टेनेसी शहर']\n",
    "\n",
    "# Transform the user input using the previously defined vectorizer\n",
    "testing = tf.transform(userInput)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "res = mn.predict(testing)\n",
    "\n",
    "# Retrieve unique contexts and answer codes from the DataFrame\n",
    "Ans = df[\"context\"].unique()\n",
    "Ans = Ans.tolist()\n",
    "Ans_Code = df[\"Answers_Code\"].unique()\n",
    "Ans_Code = Ans_Code.tolist()\n",
    "\n",
    "# Print the predictions\n",
    "print('\\n\\n\\n')\n",
    "for i in res:\n",
    "    print(Ans[Ans_Code.index(i)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
